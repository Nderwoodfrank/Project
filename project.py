# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YGuyXZiFBl9rxJ9bvnAy-7W87FAWWtBz
"""



import torch
import numpy as np
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from matplotlib import pyplot as plt
import argparse
import torchvision
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torchvision.datasets import CIFAR10, CIFAR100, MNIST
from torch.utils.data import DataLoader, random_split
from torch.optim.lr_scheduler import StepLR
import sparselearning
from sparselearning.core import add_sparse_args, CosineDecay, Masking
from sparselearning.funcs import no_redistribution, magnitude_prune, random_growth
import logging
import os
import sys

def setup_logger():
    if not os.path.exists('./logs'): os.mkdir('./logs')
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    formatter = logging.Formatter(fmt='%(asctime)s: %(message)s', datefmt='%H:%M:%S')

    log_path = './logs/training.log'
    fh = logging.FileHandler(log_path)
    fh.setFormatter(formatter)
    logger.addHandler(fh)
    return logger

logger = setup_logger()
log_interval = 100

class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, in_planes, planes, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion*planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(self.expansion*planes)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out

class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, in_planes, planes, stride=1):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(self.expansion*planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion*planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(self.expansion*planes)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out

class ResNet(nn.Module):
    def __init__(self, block, num_blocks, num_classes=10):
        super(ResNet, self).__init__()
        self.in_planes = 64

        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)
        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)
        self.linear = nn.Linear(512*block.expansion, num_classes)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1]*(num_blocks-1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out

def ResNet18(c=1000):
    model=ResNet(BasicBlock, [2,2,2,2],c)
    return model

def ResNet34(c=10):
    model=ResNet(BasicBlock, [3,4,6,3],c)
    return model

def ResNet50(c=100):
    model=ResNet(Bottleneck, [3,4,6,3],c)
    return model

transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Load CIFAR-10 dataset
trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform_train)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform_test)
testloader = torch.utils.data.DataLoader(testset, batch_size=100,
                                         shuffle=False, num_workers=2)

class SelectivePruning:
    def __init__(self, model, optimizer, density, prune_rate, sparsity_ratio):
        self.model = model
        self.optimizer = optimizer
        self.density = density
        self.prune_rate = prune_rate
        self.sparsity_ratio = sparsity_ratio
        self.logger = logging.getLogger("SelectivePruning")

        # Initialize the entire model with sparsity
        self.masking = Masking(optimizer, CosineDecay(prune_rate, 100), prune_rate, growth_mode='random', prune_mode='magnitude', redistribution_mode='none', verbose=True)
        self.masking.add_module(model, density)

        # Select the block to focus on based on magnitude
        self.selected_block_name = self.select_block_for_pruning()
        self.logger.info(f"Selected block for focused pruning and growth: {self.selected_block_name}")

    def apply_masks(self):
        """Apply masks to enforce sparsity after each optimizer step."""
        self.masking.apply_mask()

    def calculate_total_magnitude(self, module):
        """Calculate the total magnitude of weights in a module."""
        return sum(p.data.abs().sum().item() for p in module.parameters() if p.requires_grad)

    def select_block_for_pruning(self):
        """Select the block with the highest total magnitude for focused pruning."""
        block_magnitudes = {name: self.calculate_total_magnitude(module) for name, module in self.model.named_modules() if isinstance(module, (nn.Conv2d, nn.Linear))}
        return max(block_magnitudes, key=block_magnitudes.get)

    def prune_and_regrow(self):
        """Apply pruning and regrowth only to the selected block."""
        for name, module in self.model.named_modules():
            if name == self.selected_block_name:
                self.masking.prune_rate = self.prune_rate  # Set specific prune rates if needed
                self.masking.step()  # Apply pruning and potentially regrowth
                self.masking.apply_mask()

    def share_weights(self):
        """Share weights from the selected block to other blocks with matching dimensions."""
        source_block = next((m for n, m in self.model.named_modules() if n == self.selected_block_name), None)
        if source_block:
            for target_name, target_module in self.model.named_modules():
                if isinstance(target_module, type(source_block)) and target_name != self.selected_block_name:
                    if all(s == t for s, t in zip(source_block.weight.size(), target_module.weight.size())):
                        self._share_weights(source_block, target_module)


    def _share_weights(self, source_module, target_module):
        """Helper function to share weights from source to target module."""
        source_block = self.get_layer_by_name(source_module)
        target_block = self.get_layer_by_name(target_module)
        total_weights = source_module.weight.data.numel()
        weights_to_share = int(total_weights * self.sparsity_ratio)
        _, indices = torch.flatten(source_module.weight.data.abs()).topk(weights_to_share)
        mask = torch.zeros(total_weights, dtype=torch.bool, device=source_module.weight.device)
        mask[indices] = True
        mask = mask.view_as(source_module.weight.data)
        target_module.weight.data.copy_(source_module.weight.data * mask)

        if hasattr(source_module, 'bias') and source_module.bias is not None:
            target_module.bias.data.copy_(source_module.bias.data)
        self.logger.info(f"Weights shared from {source_block} to {target_block}, including biases as applicable.")

    def get_layer_by_name(self, layer_name):
        """
        Retrieve a layer from the model using its name.
        """
        for name, module in self.model.named_modules():
            if name == layer_name:
                return module
        self.logger.error(f"Layer {layer_name} not found in the model.")
        return None

    def log_non_zero_weights(self, phase):
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Conv2d):
               total_weights = module.weight.data.numel()
               non_zero_weights = torch.count_nonzero(module.weight.data).item()
               percentage_non_zero = non_zero_weights / total_weights
               self.logger.info(f"{phase} - Layerwise percentage of the fired weights of {name}.weight is: {percentage_non_zero:.10f}")

def train_epoch(model, train_loader, optimizer, device, epoch, selective_pruning):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.cross_entropy(output, target)
        loss.backward()
        optimizer.step()
        selective_pruning.apply_masks()  # Apply sparsity masks globally

        if batch_idx % 100 == 0:
            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\tLoss: {loss.item():.6f}')

def evaluate(model, test_loader, device):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.cross_entropy(output, target, reduction='sum').item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
    test_loss /= len(test_loader.dataset)
    accuracy = 100. * correct / len(test_loader.dataset)
    logging.info(f'\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.0f}%)')

def parse_arguments():
    parser = argparse.ArgumentParser(description='Train a ResNet model with selective pruning and optional weight sharing.')
    parser.add_argument('--epochs', type=int, default=10, help='Number of epochs to train.')
    parser.add_argument('--batch_size', type=int, default=128, help='Batch size for training.')
    parser.add_argument('--density', type=float, default=0.05, help='Density of the connections.')
    parser.add_argument('--prune_rate', type=float, default=0.2, help='Pruning rate.')
    parser.add_argument('--sparsity_ratio', type=float, default=0.1, help='Sparsity ratio for weight sharing.')
    parser.add_argument('--disable_weight_sharing', action='store_true', help='Disable weight sharing feature.')
    args = parser.parse_args()
    return args

def main():
    args = parse_arguments()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = ResNet50(c=100).to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)

    selective_pruning = SelectivePruning(model, optimizer, args.density, args.prune_rate, args.sparsity_ratio)
    selective_pruning.log_non_zero_weights('Before Training')

    for epoch in range(args.epochs):
        train_epoch(model, trainloader, optimizer, device, epoch, selective_pruning)
        selective_pruning.prune_and_regrow()
        if not args.disable_weight_sharing:
            selective_pruning.share_weights()
        evaluate(model, testloader, device)
        selective_pruning.log_non_zero_weights('After Training')
        scheduler.step()


if __name__ == "__main__":
    main()
